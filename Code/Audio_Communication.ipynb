{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d7fb32",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be15d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import egg.core as core\n",
    "from egg.core import Callback, Interaction, PrintValidationEvents\n",
    "from torchvision import models, datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import json\n",
    "import librosa as lr\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f330a",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5188b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGG_Dataset(Dataset):\n",
    "    '''\n",
    "    A dataset that is read in from a txt file and is compatible with the EGG framework and PyTorch's DataLoader.\n",
    "    \n",
    "    Attributes:\n",
    "        frame -- The dataset\n",
    "        \n",
    "    Methods:\n",
    "        __init__(game_data_path, test_or_val) -- Constructor, initializes a dataset object\n",
    "        get_n_features() -- Returns the dimension of an image embedding\n",
    "        __len__() -- Returns the lenght of the dataset\n",
    "        __getitem__(idx) -- Returns an element of the dataset by the given index idx\n",
    "    '''\n",
    "    def __init__(self, game_data_path, test=False):\n",
    "        '''\n",
    "        Constructor, initializes an EGG_Dataset object, i.e. a dataset that is compatible with both EGG\n",
    "        and PyTorch's DataLoader.\n",
    "        \n",
    "        Parameter:\n",
    "            game_data_path -- The path to the game data (train, val, or test) for which a dataset is to be produced\n",
    "            test -- Boolean that indicates whether or not the dataset will be used for testing\n",
    "        '''\n",
    "        self.frame = []\n",
    "        with open(game_data_path, 'r') as game_data:\n",
    "            for i,line in enumerate(game_data):\n",
    "                # splits row into list with an element for each embedding and the target idx (at last position)\n",
    "                raw_info = line.split(\",\")\n",
    "                # stores the embeddings in nested list index_vectors and casts values to int\n",
    "                # , e.g. [[feat1, feat2, feat3], [feat1, feat2, feat3]]\n",
    "                embeddings = list([list(map(float, x.split())) for x in raw_info[:3]])\n",
    "                # index of the target embedding\n",
    "                target_idx = torch.tensor(int(raw_info[-1]))\n",
    "                # the target embedding, which is the sender input\n",
    "                target_embedding = torch.FloatTensor(embeddings[target_idx])\n",
    "                # constructing the receiver input, i.e. storing target and distractor embeddings in one tensor\n",
    "                target_and_distractors = []\n",
    "                for embedding in embeddings:\n",
    "                    target_and_distractors = np.concatenate((target_and_distractors, embedding))\n",
    "                target_and_distractors = torch.FloatTensor(target_and_distractors).view(len(embeddings), -1)\n",
    "                # for training and validation data: storing sender input, target_index, and receiver input in data frame\n",
    "                if not test:\n",
    "                    self.frame.append((target_embedding, target_idx, target_and_distractors))\n",
    "                # for test data additionally store the ImageNet class of the target image\n",
    "                elif test:\n",
    "                    target_class = int(raw_info[-2])\n",
    "                    self.frame.append((target_embedding, target_idx, target_and_distractors, target_class))\n",
    "        \n",
    "    def get_n_features(self):\n",
    "        'Returns the dimension of the image embeddings'\n",
    "        return self.frame[0][0].size(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the length of the dataset'\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Returns an element of the dataset by index'\n",
    "        return self.frame[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6849451",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EGG_Dataset('./data/Imagenet_embeddings/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ef7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = EGG_Dataset('./data/Imagenet_embeddings/val.txt')\n",
    "val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EGG_Dataset('./data/Imagenet_embeddings/test.txt', test=True)\n",
    "test_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_digits_path = './data/spoken_digits/same_length/'\n",
    "\n",
    "spoken_digits_raw = []\n",
    "_, sr = lr.load(spoken_digits_path+str(0)+'.wav')\n",
    "\n",
    "for i in range(10):\n",
    "    wave, sr = lr.load(spoken_digits_path+str(i)+'.wav')\n",
    "    spoken_digits_raw.append(wave)\n",
    "\n",
    "digit_audios = torch.Tensor(spoken_digits_raw)\n",
    "digit_audios = torch.transpose(digit_audios, 0, 1)\n",
    "digit_audios = digit_audios.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3151b1",
   "metadata": {},
   "source": [
    "### Implementing the Sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00da89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    '''\n",
    "    The core of any sender agent. It takes the embedding of the target image as input and produces an initial\n",
    "    hidden state for the message producing GRU (that will be initialized via an EGG-Wrapper below).\n",
    "    \n",
    "    Attributes:\n",
    "        fc1 -- The only layer of the sender's core\n",
    "    \n",
    "    Methods:\n",
    "        __init__(n_hidden, n_features) -- Constructor, initializes the sender's core\n",
    "        forward(x, _aux_input) -- Performs a forward pass through the sender's core \n",
    "    '''\n",
    "    def __init__(self, n_hidden, n_features):\n",
    "        '''\n",
    "        Constructor, initializes the sender's core.\n",
    "        \n",
    "        Parameter:\n",
    "            n_features -- The input size, i.e. the number of elements in an image embedding\n",
    "            n_hidden -- The output size, i.e. the size of the hidden states in the message producing GRU\n",
    "            \n",
    "        Output:\n",
    "            None\n",
    "        '''\n",
    "        super(Sender, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        '''Performs a forward pass through the sender's core, i.e. maps the target img embedding x to \n",
    "        the initial hidden state of the GRU and returns this mapping'''\n",
    "        return self.fc1(x)\n",
    "        return self.fc1(x).tanh()\n",
    "        #return self.fc1(x).LeakyReLu\n",
    "        # here, it might make sense to add a non-linearity, such as tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71172e23",
   "metadata": {},
   "source": [
    "### Implementing the Receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df98ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver(nn.Module):\n",
    "    '''\n",
    "    The core of any receiver agent. It takes two inputs: a message embedding from its wrapper GRU and the image\n",
    "    embeddings of the target and distractors. Target and distractor embeddings are mapped to the same dimension\n",
    "    as the message embedding and then computes the dot product between the message embedding and each of the mapped\n",
    "    image embeddings. The resulting list of dot products is interpreted as a non-normalized probability distribution\n",
    "    over possible target positions, i.e. the highest dot product was computed using the mapped embedding of the target image.\n",
    "    \n",
    "    Attributes:\n",
    "        fc1 -- The only layer of the receiver's core\n",
    "        \n",
    "    Methods:\n",
    "        __init__(n_features, n_hidden) -- Constructor, initializes a receiver's core\n",
    "        forward(x, _input, _aux_input) -- Performs a forward pass through the receiver's core\n",
    "    '''\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        '''\n",
    "        Constructor, initalizes the sender's core.\n",
    "        \n",
    "        Parameter:\n",
    "            n_features -- The input sizes of the target and distractor embeddings\n",
    "            n_hidden -- The size of the wrapper-GRU's hidden state\n",
    "            \n",
    "        Output:\n",
    "            None\n",
    "        '''\n",
    "        super(Receiver, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "\n",
    "    def forward(self, x, _input, _aux_input):\n",
    "        '''\n",
    "        Performs a forward pass through the receiver's core, i.e. maps all embeddings of target and distractor\n",
    "        images to the dimension of the wrapper-GRU's message embedding and then computes and returns the dot\n",
    "        products between all target/distractor mappings and the message embedding.\n",
    "        \n",
    "        Parameter:\n",
    "            x -- The message embedding produced by the wrapper-GRU\n",
    "            _input -- The embeddings of target and distractor images\n",
    "        \n",
    "        Output:\n",
    "            dots -- A list of dot products between mapped image embeddings and the message embedding, the element\n",
    "                    with the highest dot product acts as the receiver's prediction of the target position\n",
    "        '''\n",
    "        # the rationale for the non-linearity here is that the RNN output (x) will also be the outcome of a non-linearity\n",
    "        embedded_input = self.fc1(_input).tanh()\n",
    "        dots = torch.matmul(embedded_input, torch.unsqueeze(x, dim=-1))\n",
    "        #return dots.squeeze()\n",
    "        return F.softmax(dots.squeeze(), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaceabe3",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4509cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game parameter\n",
    "vocab_size = 10\n",
    "max_len = 5\n",
    "\n",
    "# agent parameter\n",
    "sender_hidden = 10\n",
    "receiver_hidden = 35\n",
    "sender_embedding = 5\n",
    "receiver_embedding = 5\n",
    "sender_cell = 'gru'\n",
    "receiver_cell = 'gru'\n",
    "n_features = test_dataset.get_n_features()\n",
    "\n",
    "# training parameter, defined on egg level for optimizer call in game initialization\n",
    "opts = core.init(params=['--lr=1e-4',\n",
    "                         '--batch_size=128',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "# setting a PyTorch seed\n",
    "seed = 4\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8abd20",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=0, drop_last = True, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=0, drop_last = True, pin_memory=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=0, drop_last = True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f0d7cc",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(_sender_input, _message, _receiver_input, receiver_output, labels, _aux_input):\n",
    "    '''\n",
    "    The loss function used for optimizing the game agents. EGG requires the loss function to have all these\n",
    "    parameters even if they are not used for the loss computation.\n",
    "    \n",
    "    Parameter (all parameters have on value for each element in the batch):\n",
    "        _sender_input -- The input to the sender (an image embedding), not used for my loss computation\n",
    "        _message -- The message produced by the sender, not used for my loss computation\n",
    "        _receiver_input -- The input to the receiver (target + distractors), not used for my loss computation\n",
    "        receiver_output -- The output of the receiver\n",
    "        labels -- The indexes of the target images\n",
    "        \n",
    "    Output:\n",
    "        loss, {'acc': acc}\n",
    "            loss -- The cross entropy between the receiver output and the labels\n",
    "            acc -- The success rates (0 or 1) of all samples in the batch\n",
    "    '''\n",
    "    global batch_accs, batch_losses, validate\n",
    "    # in the discriminative case, accuracy is computed by comparing the index with highest score in Receiver output (a distribution of unnormalized\n",
    "    # probabilities over target poisitions) and the corresponding label read from input, indicating the ground-truth position of the target\n",
    "    acc = (receiver_output.argmax(dim=1).to(device) == labels.to(device)).detach().float()\n",
    "    # similarly, the loss computes cross-entropy between the Receiver-produced target-position probability distribution and the labels\n",
    "    loss = F.cross_entropy(receiver_output.to(device), labels.to(device), reduction=\"none\")\n",
    "    batch_accs.append(torch.mean(acc).item())\n",
    "    batch_losses.append(torch.mean(loss).item())\n",
    "        \n",
    "    \n",
    "    return loss, {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829a970",
   "metadata": {},
   "source": [
    "### Set Up the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7946424",
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(n_hidden=sender_hidden, n_features=n_features).to(device)\n",
    "sender = core.RnnSenderReinforceAudio(\n",
    "                            sender,\n",
    "                            vocab_size=vocab_size,\n",
    "                            embed_dim=sender_embedding,\n",
    "                            hidden_size=sender_hidden,\n",
    "                            audio_data=digit_audios,\n",
    "                            cell=sender_cell,\n",
    "                            max_len=max_len,\n",
    "                            ).to(device)\n",
    "receiver = Receiver(n_features=n_features, n_hidden=receiver_hidden).to(device)\n",
    "receiver = core.RnnReceiverDeterministicAudio(\n",
    "                            receiver,\n",
    "                            vocab_size=vocab_size,\n",
    "                            embed_dim=receiver_embedding,\n",
    "                            hidden_size=receiver_hidden,\n",
    "                            cell=receiver_cell,\n",
    "                            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15457713",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = core.SenderReceiverRnnReinforceAudio(\n",
    "                            sender,\n",
    "                            receiver,\n",
    "                            loss,\n",
    "                            sender_entropy_coeff=0,\n",
    "                            receiver_entropy_coeff=0,\n",
    "                            ).to(device)\n",
    "current_callbacks = []\n",
    "current_optimizer = core.build_optimizer(game.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = core.Trainer(\n",
    "            game=game,\n",
    "            optimizer=current_optimizer,\n",
    "            train_data=train_loader,\n",
    "            callbacks=current_callbacks+ [core.ConsoleLogger(print_train_loss=True, as_json=True)],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ac9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train(egg_trainer):\n",
    "    global batch_accs, batch_losses\n",
    "    batch_accs = []\n",
    "    batch_losses = []\n",
    "    egg_trainer.train(n_epochs = 1)\n",
    "\n",
    "    return (np.array(batch_accs).mean(), np.array(batch_losses).mean())\n",
    "'''\n",
    "def check_performance(dataset):\n",
    "    '''\n",
    "    Checks the communities performance on the provided dataset. To do so, a pair is sampled from the community\n",
    "    for each batch in the dataset. That pair's loss and accuracy are computed and stored.\n",
    "    \n",
    "    Parameter:\n",
    "        dataset -- The dataset for which the communities performance is to be tested.\n",
    "        \n",
    "    Output:\n",
    "        (accuracy, loss)\n",
    "            accuracy -- The average accuracy achieved by the sampled pairs on the dataset's batches.\n",
    "            loss -- The average loss achieved by the sampled pairs on the dataset's batches.\n",
    "    '''\n",
    "    global batch_accs, batch_losses\n",
    "    batch_accs = []\n",
    "    batch_losses = []\n",
    "    \n",
    "    \n",
    "    for batch in dataset:\n",
    "        try:\n",
    "            (sender_inputs, target_idxs, receiver_inputs, target_classes) = batch\n",
    "        except ValueError:\n",
    "            (sender_inputs, target_idxs, receiver_inputs) = batch\n",
    "            \n",
    "        with torch.no_grad():\n",
    "\n",
    "            #messages, __sen_logprobs, __sen_entropies = sender(sender_inputs.to(device))\n",
    "            #rec_outputs, __rec_logrobs, __rec_entropies = receiver(messages.to(device), receiver_inputs.to(device), aux_input=None)\n",
    "            messages, audio_messages, __sen_logprobs, __sen_entropies, lengths = sender(sender_inputs.to(device))\n",
    "            rec_outputs, __rec_logrobs, __rec_entropies = receiver(audio_messages.to(device), receiver_inputs.to(device), aux_input=None, lengths=lengths)\n",
    "            loss_fn_output = loss(sender_inputs, messages, receiver_inputs, rec_outputs, target_idxs, None)\n",
    "        \n",
    "    return (np.array(batch_accs, dtype=float).mean(), np.array(batch_losses, dtype=float).mean())\n",
    "\n",
    "def visualize(train_accs, train_losses, val_accs, val_losses, results_dir):\n",
    "    '''\n",
    "    Plots the communities performance on the train and on the validation dataset.\n",
    "    \n",
    "    Parameter:\n",
    "        train_accs -- The accuracies on the training data achieved by the community for all completed epochs\n",
    "        train_losses -- The losses on the training data achieved by the community for all completed epochs\n",
    "        val_accs -- The accuracies on the validation data achieved by the community for all completed epochs\n",
    "        val_losses -- The losses on the validation data achieved by the community for all completed epochs\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "        Prints the plots in the console\n",
    "    '''\n",
    "    total_nr_epochs = len(train_accs)\n",
    "    #clear_output(wait = True)\n",
    "    plt.figure(figsize = (15,7))\n",
    "    line1, = plt.plot(range(0,len(train_losses)),train_losses)\n",
    "    line2, = plt.plot(range(0,len(val_losses)),val_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xlim(0,total_nr_epochs)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend((line1,line2),(\"training\",\"validation\"))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(results_dir+'train_and_val_losses.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (15,7))\n",
    "    line1, = plt.plot(range(0,len(train_accs)),train_accs)\n",
    "    line2, = plt.plot(range(0,len(val_accs)),val_accs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xlim(0,total_nr_epochs)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0,1.05)\n",
    "    plt.yticks(np.arange(0, 1.05, 0.05))\n",
    "    plt.legend((line1,line2),(\"training\",\"validation\"))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(results_dir+'train_and_val_accs.png')\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_training(train_accs, train_losses):\n",
    "    '''\n",
    "    Plots the communities performance on the train and on the validation dataset.\n",
    "    \n",
    "    Parameter:\n",
    "        train_accs -- The accuracies on the training data achieved by the community for all completed epochs\n",
    "        train_losses -- The losses on the training data achieved by the community for all completed epochs\n",
    "        val_accs -- The accuracies on the validation data achieved by the community for all completed epochs\n",
    "        val_losses -- The losses on the validation data achieved by the community for all completed epochs\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "        Prints the plots in the console\n",
    "    '''\n",
    "    total_nr_epochs=len(train_accs)\n",
    "    \n",
    "    plt.figure(figsize = (15,7))\n",
    "    line1, = plt.plot(range(0,len(train_losses)),train_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xlim(0,total_nr_epochs)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Across Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (15,7))\n",
    "    line1, = plt.plot(range(0,len(train_accs)),train_accs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xlim(0,total_nr_epochs)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Across Epochs\")\n",
    "    plt.ylim(0,1.05)\n",
    "    plt.yticks(np.arange(0, 1.05, 0.05))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f88e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_start = time.time()\n",
    "global batch_accs, batch_losses\n",
    "\n",
    "n_epochs = 200\n",
    "epoch_train_accs = []\n",
    "epoch_train_losses = []\n",
    "epoch_val_accs = []\n",
    "epoch_val_losses = []\n",
    "start_train_accs, start_train_losses = check_performance(train_loader)\n",
    "start_val_accs, start_val_losses = check_performance(val_loader)\n",
    "epoch_train_accs.append(start_train_accs)\n",
    "epoch_train_losses.append(start_train_losses)\n",
    "epoch_val_accs.append(start_val_accs)\n",
    "epoch_val_losses.append(start_val_losses)\n",
    "for i in range(n_epochs):\n",
    "    batch_accs = []\n",
    "    batch_losses = []\n",
    "    print('Epoch: ', i+1)\n",
    "    trainer.train(n_epochs = 1)\n",
    "    epoch_train_accs.append(np.mean(np.array(batch_accs)))\n",
    "    epoch_train_losses.append(np.mean(np.array(batch_losses)))\n",
    "    val_accs, val_losses = check_performance(val_loader)\n",
    "    epoch_val_accs.append(val_accs)\n",
    "    epoch_val_losses.append(val_losses)\n",
    "    \n",
    "total_time_end = time.time()\n",
    "total_time = total_time_end - total_time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './Results/Audio/Seed_'+str(seed)+'/'\n",
    "print(epoch_train_accs)\n",
    "print(epoch_train_losses)\n",
    "print(epoch_val_accs)\n",
    "print(epoch_val_losses)\n",
    "visualize(epoch_train_accs, epoch_train_losses, epoch_val_accs, epoch_val_losses, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb792bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = check_performance(test_loader)\n",
    "print('Accuracy on the test dataset:', test_acc)\n",
    "print('Loss on the test dataset:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2971e5",
   "metadata": {},
   "source": [
    "### Storing accuracies and losses during training, test accuracy and loss and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the train accs of all epochs\n",
    "with open(results_dir+'train_accs.txt', 'w') as f:\n",
    "    json.dump(epoch_train_accs, f)\n",
    "\n",
    "# saving the train losses of all epochs\n",
    "with open(results_dir+'train_losses.txt', 'w') as f:\n",
    "    json.dump(epoch_train_losses, f)\n",
    "\n",
    "# saving the validation accuracies of all epochs\n",
    "with open(results_dir+'val_accs.txt', 'w') as f:\n",
    "    json.dump(epoch_val_accs, f)\n",
    "\n",
    "# saving the validation losses of all epochs\n",
    "with open(results_dir+'val_losses.txt', 'w') as f:\n",
    "    json.dump(epoch_val_losses, f)\n",
    "    \n",
    "# saving the test accuracy\n",
    "with open(results_dir+'test_acc.txt', 'w') as f:\n",
    "    json.dump(test_acc, f)\n",
    "\n",
    "# saving the test loss\n",
    "with open(results_dir+'test_loss.txt', 'w') as f:\n",
    "    json.dump(test_loss, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfef5ce",
   "metadata": {},
   "source": [
    "### Producing the evaluation data (The messages produced for each sample in the test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_dir+'evaluation_data.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['id', 'sender_input', 'target_class', 'accuracy', 'message']\n",
    "    writer.writerow(header)\n",
    "    row_nr = 0\n",
    "    \n",
    "    for (sender_inputs, target_idxs, receiver_inputs, target_classes) in test_loader:\n",
    "        current_batch_size = len(target_idxs)\n",
    "        # messages contains one entry for each sender and each of these entries is a batch of messages\n",
    "        #messages, audio_messages, _batch_sen_logprobs, _batch_sen_entropies, lengths = sender(sender_inputs.to(device))\n",
    "        messages, audio_messages, __sen_logprobs, __sen_entropies, lengths = sender(sender_inputs.to(device))\n",
    "        # contains one list for each batch of messages. These lists contain one list from each receiver\n",
    "        # corresponding to the receiver's outputs given that message batch. In other words, the first\n",
    "        # element contains the receiver outputs of all pairs involving sender1, the second one of all pairs\n",
    "        # involving sender2 and so on.\n",
    "        #rec_outputs, _batch_rec_logrobs, _batch_rec_entropies = receiver(audio_messages, receiver_inputs.to(device))\n",
    "        rec_outputs, __rec_logrobs, __rec_entropies = receiver(audio_messages.to(device), receiver_inputs.to(device), aux_input=None, lengths=lengths)\n",
    "        # one row for each element in the batch\n",
    "        for i in range(current_batch_size):\n",
    "            row_nr += 1\n",
    "            sender_input = str(sender_inputs[i].tolist()).replace('[', '').replace(']', '')\n",
    "            target_class = target_classes[i].item()\n",
    "            row = [row_nr, sender_input, target_class]\n",
    "            \n",
    "            sample_target_idx = target_idxs[i]\n",
    "            rec_output = rec_outputs[i]\n",
    "            sample_acc = int(rec_output.argmax(dim=0)==sample_target_idx)\n",
    "            row.append(sample_acc)\n",
    "            message = str(messages[i].tolist()).replace('[', '').replace(']', '')\n",
    "            row.append(message)\n",
    "\n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
